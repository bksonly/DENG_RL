### 项目规范：“干瞪眼”扑克牌游戏的强化学习实现

请从零开始实现一个完整的强化学习项目，目标是训练一个能玩中国扑克牌游戏“干瞪眼”的 Agent。

**核心约束条件：**

1. **依赖库：** 仅使用 `numpy` 和 `torch`。**严禁**使用 `gym` 或 `gymnasium`。
2. **文件结构：** 代码必须拆分为且仅包含两个文件：`game_env.py`（游戏逻辑）和 `train_ppo.py`（RL 算法）。
3. **硬件适配：** 代码必须针对笔记本电脑（CPU 训练）进行优化（减少不必要的开销），但如果检测到 CUDA 也应支持 GPU 加速。

---

### 第一部分：游戏规则与逻辑 (`game_env.py`)

**游戏概览：**
“干瞪眼”是一种“跑得快”类型的游戏（谁先出完手牌谁赢）。

* **牌堆：** 54 张牌（52 张标准牌 + 2 张王）。
* **大小顺序：** 3 < 4 < 5 < 6 < 7 < 8 < 9 < 10 < J < Q < K < A < 2 。
* **“大一点”规则（核心机制）：** 你出的牌（或牌型）必须比上家的牌点数**刚好大 1 点**，除非是 2 或 2 对。
* *例子：* 上家出 4，你必须出 5。你不能出 6 或更大的牌。
* *例外 1（2）：* “2” 是特殊牌。它可以管住任何普通的单张或对子（除了王和炸弹）。普通牌（如 3）不能管住 2。只有王或炸弹能管住 2。
* *例外 2（王）：* 王是赖子（万能牌）。它们可以代替任何牌来凑成对子、顺子或炸弹。但是大小王不能单出，这意味着如果手牌里只剩一张大小王，则无法出掉。


* **牌型：**
1. **单张：** 例如 3。
2. **对子：** 例如 3-3 或 3-王。
3. **顺子：** 3 张或更多连续的牌（例如 3-4-5）。*规则：接牌时必须点数严格 +1。*
4. **炸弹：** 3 张或更多同点数的牌（例如 3-3-3）。炸弹可以管住任何非炸弹牌型。
更大的炸弹可以管住小的炸弹，不含大小王的炸弹大于含大小王的炸弹，炸弹不需要遵守大一点规则。

* **胜利条件：** 第一个出完手牌的玩家获胜。

**类定义：`GanDengYanEnv**`
不需要继承 gym。请实现以下方法：

1. `__init__()`:
* 初始化牌堆。一副牌，总共四个玩家。这副牌的排列顺序。每局随机指定一个玩家为庄家。初始发 5 张，庄家发 6 张。庄家先出牌。
* **动作空间生成 (Action Space Generation)：** 生成一个包含**所有**可能出牌动作（单张、对子、顺子、炸弹、过）的静态列表。将它们映射为索引 `0` 到 `N`。这就是我们的离散动作空间。


2. `reset()`:
* 洗牌、发牌。返回初始观测值 (Observation)。


3. `get_legal_actions(hand, last_move)`:
* **关键函数：** 返回一个布尔类型的 Mask（长度为 `N`）。
* 逻辑：识别当前手牌中所有能管住 `last_move` 的合法动作。
* *注意：* 正确处理赖子（王）的排列组合逻辑（例如：[3, 王] 可以被视为 对3）。


4. `step(action_idx)`:
* 更新游戏状态。
* **奖励 (Reward)：** 采用稀疏奖励，只在终局结算。
* 假设本局有唯一赢家 `W`，其他为输家。对每个输家 `i`：
  * 令 `loss_i = 剩余手牌数_i`。
  * 如果该玩家本局**通关**（整局没有实际出过牌）：`loss_i *= 2`。
    * 对于庄家，如果只出了开局的那一手起牌，之后再也没有出过牌，也视为通关。
  * 如果该玩家在终局时剩余手牌中**仍然包含任意大小王**（小王或大王任意一张即可），则在上述基础上再额外翻倍一次：`loss_i *= 2`。
  * 如果本局对战过程中**总共打出了 `k` 次炸弹**（任意玩家用炸弹管住桌面牌算作一次），则在上述基础上对**所有输家统一**再额外乘以 `2^k`：
    * 打出 1 次炸弹，相当于全场输家再翻一倍；
    * 打出 2 次炸弹，相当于在前面基础上再翻两倍（总共 4 倍），以此类推。
  * 然后对个人封顶：`loss_i = min(loss_i, 20)`。
* 若“我”是赢家：`reward = sum_i loss_i`（所有输家的 `loss_i` 之和）。
* 若“我”是输家：`reward = -loss_me`（只看自己这一家的输分）。
* 过程中其他 step 的奖励均为 `0`，不使用微弱引导奖励。
* **Pass 规则：**
  * 一般情况下（非强制首出），`过牌/pass` 是一个合法动作。
  * 当我压住上家，一圈下来无人能接，上轮由我获胜并从牌堆摸一张牌后首出时，这一手属于“摸牌后首出”，此时必须出牌，不能 pass。
* **摸牌与牌堆耗尽：**
  * 每当一轮中最后出牌的玩家获胜，且之后所有玩家都 pass，则清空当前桌面牌型，由该玩家开始新一轮。
  * 若该玩家是“我”，并且牌堆中仍有牌，则在新一轮首出前先从牌堆摸一张牌。
  * 如果在游戏过程中牌堆被摸空且牌局仍未自然结束，则本局视为无效局，立即终止，本局数据不参与训练更新（例如通过 `info['aborted'] = True` 标记）。

一个step就是一轮，也就是“我”出完一次牌，到下一次轮到"我”出。对手的策略做成可选的，训练早期是贪心，后期可以改成ppo policy，也就是自己和自己对抗。
* 返回：`next_state`, `reward`, `done`, `info`。


5. `state_encoding(hand, public_info)`:

建议将 **可卷积的牌面信息** 设计为一个 `[C_s, 4, 15]` 的张量（Tensor），类似于图像处理中的“多通道图片”；同时，将一些全局标量信息单独编码为一个小向量 `global_feats`。  
其中：
* 15：牌的大小（3 ~ 4 ~ … ~ K ~ A ~ 2 ~ 小王 ~ 大王）。
* 4：牌的数量编码（0, 1, 2, 3+），在“牌矩阵”通道中表示该点数是否至少有 1/2/3/4(3+) 张。
* `C_s`：**空间通道数**，只用于表示真正和牌面布局相关、适合卷积的矩阵信息。

空间通道设计（示例约定）：

* **Channel 0 (My Hand)：** 我手里的牌。  
  用一个 `[4, 15]` 的数量编码矩阵表示当前手牌。

* **Channel 1 (The "Graveyard" - 全局已出牌)：**  
  一个 `[4, 15]` 的数量编码矩阵，记录目前为止桌面上所有打出去的牌的总和，用于“算牌”（外面还剩多少炸弹、还有没有 2 等）。

* **Channel 2 (The "Target" - 当前必须管的牌)：**  
  一个 `[4, 15]` 的数量编码矩阵，表示当前桌面上我需要去“管”的那一手牌型（上一家或上一轮的出牌）。  
  如果是我从头开始出牌（包括压制上家后一圈无人能接、我摸牌后获得新一轮首出权的情况），这里全 0。

除此之外，再单独维护一个 **全局标量特征向量**：

* `global_feats`：长度为 5 的向量，例如可以按如下顺序编码：
  1. 我自己剩余手牌数（或其归一化版本，例如除以 20 并截断到 `[0,1]`）。  
  2. 左家剩余手牌数（归一化）。  
  3. 对家剩余手牌数（归一化）。  
  4. 右家剩余手牌数（归一化）。  
  5. `is_first_move_after_draw`：是否处于“摸牌后首出”的状态（0/1 标量）。  

> 注意：`[C_s, 4, 15]` 只包含可观测的、适合理解为“图像”的牌面信息，而对手剩余手牌数、是否为摸牌后首出等全局信息放入 `global_feats` 中；对历史顺序和隐藏信息的记忆交给 PPO 网络中的 LSTM `hidden_state` 来处理。
---

### 第二部分：RL 算法 (`train_ppo.py`)

**算法规范：**

* **算法：** 针对离散动作空间的 **PPO (Proximal Policy Optimization)**。
* **网络架构：**
* **输入：**  
  - 空间输入：来自 `game_env` 的牌面矩阵编码 `state_spatial`，形状为 `[C_s, 4, 15]`；  
  - 全局输入：一小段标量特征 `global_feats`，形状为 `[5]`，包含自己和三个对手的剩余手牌数（归一化）以及 `is_first_move_after_draw` 标志。  
* **骨干网络 (Backbone)：**  
  - 一条 **CNN 分支** 处理 `state_spatial`，提取牌面空间特征；  
  - 一条 **MLP 分支** 处理 `global_feats`，将 5 维标量映射到一个较高维的表示（例如 32 维）。  
* **特征融合与记忆模块：**  
  - 将 CNN 输出的特征与 MLP 输出的全局特征在特征维度拼接，作为整体特征向量；  
  - 使用 **LSTM 层** 处理该整体特征序列，用于建模对局历史和隐藏信息。  
* **输出头 (Heads)：**
* `Actor`：输出所有 `N` 个动作的 Logits。**在此处应用 Action Masking**（在 Softmax 之前将非法动作的 Logits 设为 `-1e9`）。
* `Critic`：输出标量 Value。





**类结构：**

1. `ActorCritic(nn.Module)`:
* 必须能处理 `(state_spatial, global_feats, hidden_state)` 的输入，并返回新的 `hidden_state`。  
  其中 `state_spatial` 形状为 `[B, C_s, 4, 15]`，`global_feats` 形状为 `[B, 5]`。


2. `PPO_Agent`:
* `select_action(state_spatial, global_feats, legal_mask, hidden)`: 返回 action, log_prob, value, new_hidden。
* `update(memory)`: 标准的 PPO 更新逻辑，包含 Clip Loss 和 Entropy Loss。


3. `VectorEnv` (简易封装):
* 由于不使用 gym，请实现一个简单的类，通过列表循环（List Loop）来运行 `num_envs` 个 `GanDengYanEnv` 实例（串行执行对于笔记本 CPU 来说足够高效）。



**主循环 (Main Loop)：**

* 初始化 `VectorEnv`（例如并行 4 个环境）。
* 运行 `step` 收集轨迹数据（包含 States, Actions, Rewards, Masks, Dones, Hidden States）。
* 每隔 `update_timestep` 步执行一次 PPO 更新。
* 打印奖励和胜率日志。

---

### 代码实现注意事项

* **性能优化：** 在 `game_env.py` 中，查找含赖子（王）的合法动作组合可能比较慢。请使用高效的集合/组合逻辑。
* **数据类型：** 确保输入网络的 `state` 转换为 `torch.float`，`action` 转换为 `torch.long`。
* **简洁性：** 保持代码可读性。不要使用外部配置文件，目前将所有超参数硬编码在 `train_ppo.py` 顶部即可。

**请根据以上规范，生成 `game_env.py` 和 `train_ppo.py` 的完整代码。**